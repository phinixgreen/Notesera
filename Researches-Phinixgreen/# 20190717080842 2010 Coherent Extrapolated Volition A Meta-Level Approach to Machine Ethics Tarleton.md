# \# 20190717080842 2010 Coherent Extrapolated Volition A Meta-Level Approach to Machine Ethics Tarleton

\# \# 20190717080842 2010 Coherent Extrapolated Volition: A Meta-Level Approach to Machine Ethics Tarleton\
\# 20190717080842 2010 Coherent Extrapolated Volition: A Meta-Level Approach to Machine Ethics Tarleton\
tags= Tarleton, CEV, Superintelligence, 2010, Ethics, Machine-Ethics\
Pdf= \[https://www.xodo.com/app/\#/view/642ef25a-83b1-4615-b979-73e3bb59fe9e\](https://www.xodo.com/app/\#/view/642ef25a-83b1-4615-b979-73e3bb59fe9e)\
\# Formatted Reference\
Tarleton, N., 2010. Coherent extrapolated volition: A meta-level approach to machine ethics. Machine Intelligence Research Institute.\
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

Abstract\
Tarleton describes a solution proposed by Yudkowsky (2004) to the problem of what goals to give to agents that have human-level intelligence. The paper discusses suggested method for creating artificial moral agents and describe underspecified or uncertain areas.

\#\# 1. The Difficulty of Machine Ethics

Any system with self-improving capacities may require great precaution when testing\
AI has wide range of options available to it, thus testing it in constrained environment cannot assure that it will be safe when released (Yudkowsky 2008)\
Value Lock in problem: Omuhundro (2007) argues that AIs will try to protect themselves from being shut down. SI will resist attempts to alter its values or shut it down.\
Winner takes it all: A superintelligent entity will prevent the creation / existence of other superintelligent entity. Thus it will lead to the superintelligence singleton (Bostrom 2006)

\#\# 2. Sources of Normative Fallibility

Simple models of rationality assume that agents have precisely specified goals and know them, though of course they may be greatly uncertain about the best means to achieve their goals.

Humans differ from this model because---

\* Subjective Normative Uncertainty: Individuals do not have full knowledge of their preference and values.

\* Incoherent, Context-dependent judgements: Individuals display incoherent context dependent judgement based on their changing mental states.

\* Post-hoc justifications: Many moral actions of individuals are post-hoc justifications of innate, non-conscious moral instincts.

\* Limited Domains: Building a Superintelligence will require perfect generalisation of our preferences.

\#\# 3. Social Choice and Bootstrapping

Individuals have their own preferences as well as the social collective. These often do not go together. The designers of AI must not decide what morality AI's should have, rather leave it to the social collective. However, the AI should protect the interest of both the social collective and the individuals.

\#\# 4. Coherent Extrapolated Volition

Yudkowsky (2004) suggests a bootstrapping process for SI's value system.

\> "the first superintelligence \[should\] be given the goal of extrapolating human moral change under a process of idealization, letting the idealized humanity deliberate and reflect on and modify itself and this process. \"

This means that the first SI will provide morally better pathways for humanity, and then humanity can reflect on the pathway and modify this process.

\#\# CEV Notable features

\* CEV addresses content not structure.

\* CEV is an initial dynamic. The intention is not to create an Artificial Moral Agent that acts all the time to humanity's volition, but it extrapolates the volition and acts upon it.

\* Although, an initial theory is required for extrapolation, convergence can be tested from different choices of initial conditions.

\* The success of CEV depends on sufficient coherence of extrapolation. This means that it should have clear preferences, converged within and between possibilities and provide guide for action.

5\. Comparative Analysis

6\. Open Questions

\* Meta-algorithm: Goals for the AI will harvested at run-time rather than explicitly programmed before run-time. Justification for this is to have AMA's action grounded in human preferences that are complex and opaque, therefore our reports are unreliable.

\* Factually correct beliefs: Factually correct beliefs provide realistic picture of ontology and therefore better moral preferences. AI may be able to correct factually false beliefs.

\* Singleton: Only one superintelligence AMA should be constructed because multiple singleton provides no inherent advantages for humans

\* Reflection: Individual or group preferences are reflected upon and revised in the Rawls' reflective equilibrium. Helps bootstrap theory of social choice.

\* Preference aggregation: The set of preference of a whole group are to combined somehow.

\#2010\# \#CEV \#Machine-Ethics \#Ethics \#Tarleton \#Superintelligence
