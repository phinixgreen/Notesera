# \# 20200323174649 Mind as Computing Machine (Kim 2011, p.139-165) (Part 3)

\# Pebble+ Public\
20200323174649

\* Access keys helps you navigate our system. Our access key is the Alt key. Don\'t know what an access key is? Press Enter to find out.\
\* Go to the navigation of your portfolio (Press Alt key and n)\
\* Toggle between preview and edit (Press Alt key and p)

\[image:BD3809E4-EAFB-40BA-B24B-9C123A7DA140-27513-000010D631FD1947/Pebbles.jpg\]

Mind as Computing Machine (Kim 2011, p.139-165) (Part 3)

This post summarises the Computational theory of Mind and relevant discussions

\~\*\*Machine Functionalism: Further Issues\*\*\~

\*\*Multiple realisability\*\*

Two systems S1 and S2 have same mental states at t1

S1 and S2 are physical systems and they can be of different configurations.

S1 = Biological System

S2 = Electronic System

So say that S1 and S2 have same mental states at t1 is that

S1 and S2 have similar computational function going on even though S1 and S2 are made of different stuff.

But if S1 and S2 has the same Turing Machine M,

Then S1 and S2 are identical physical realisations of M.

But it does not follow that S1 and S2 are identical.

For S2 to be identical with S1, S2 has to have, in addition to psychological configuration, similar physical configuration to S1.

\> "\
\> "These considerations give credence to the idea that in order to have genuine mentality, a system must be embedded in a natural environment (ideally including other systems like it), interacting and coping with it and behaving appropriately in response to the new, and changing, conditions it encounters."

\> (Kim 2011, p.154)

\* when machine functionalists say a system has mentality \*it has to be appropriately complex.\* But if simple Turing machines do not have proto-mentality, how can complex systems generate mentality?\
\* Alan Turing proposed the \*\*Turing test\*\* to test if Machines have \*\*mentality\*\*

\~\*\*CAN MACHINES THINK? THE TURING TEST\*\*\~

\*\*The Turing Test\*\*

For a machine to prove that it can think on its own, Turing proposed a test where a machine, hidden from appearance to the examiner, has to fool the examiner through a dialogue into thinking that it is a human being.

\*\*The Objections\*\*

1\. \*\*Tough:\*\* It is impossible to design such a machine. The bottom-up approach, or where you design a machine wholly to have a discussion by answering a universal set of answers is impossible.\
2. \*\*Narrow:\*\* Proving that a machine is like a human is a narrow category for having mentality. A dog or octopus can have a mentality as well.

The Turing test is designed to test the dialogic competency of mentality.

It does not, however, cover, all aspects of mentality such as feeling emotions or feeling pain.

\*"Turing's Thesis. If two systems are input-output equivalent, they have the same psychological status; in particular, one is mental, or intelligent, just in case the other is." (Kim 2011, p. 158)\*

This obviously implies that two systems, by virtue of having input-output equivalent and having the same psychological status, cannot be two distinct things but one distinct system.

Turing test only takes into consideration the inputs and outputs of a system.

But it seems "Internal processing ought to make a difference to mentality." (Kim 2011, p. 159)

\*\*Verdict:\*\* The behavioural test is inadequate for proving a machine has the mentality or inner life.

\*"That designing a "thinking" machine that will pass the Turing test has not been a priority for artificial-intelligence researchers for the past several decades." (Kim 2011, p. 159)\*

\*\*COMPUTATIONALISM AND THE "CHINESE ROOM"\*\*

\> "\
\> "Computationalism, or the computational theory of mind, is the view that cognition, human or otherwise, is information processing, and that information processing is computation over symbolic representations according to syntactic rules, rules that are sensitive only to the shapes of these representations."

\> (Kim 2011, p. 160)

\*"On this view, mental events, states, and processes are computation events, states, and processes, and there is nothing more to a cognitive process than what is captured in a computer program successfully modeling it." - (Kim 2011, p. 160)\*

In other words,

\*"the mind as a digital computer that stores and manipulates symbol sequences according to fixed rules of transformation."- (Kim 2011, p. 160)\*

\*\*John Searle's Strong AI\*\*

\*" According to strong AI, the computer is not merely a tool in the study of the mind; rather, the appropriately programmed computer really is a mind, in the sense that computers given the right programs can be literally said to understand and have other cognitive states." - (Kim 2011, p. 160)\*

\*\*Searle's Chinese room Argument\*\*

In the Chinese Room experiment, a person who knows no Chinese is confined to a room and is given a set of instructions. He is passed messages in Chinese through a hole and he follows the instructions to respond in Chinese through the hole. This way, he responds to external stimuli without even understanding what it is about.

\*\*Thoughts\*\*

The person in the chinese room is reading symbols without perceiving them. Thus there is a causal gap in the understadning of the stimuli. It proves that behavior is possible without really feeling stimuli, or stimuli is misrepresented here.

The person in the room could be a computer and still do the job very well. It comes to that, the computer is manipulating symbols without understanding them.

\~\*\*Objections against the Computational Theory of Mind\*\*\~

On the computational view of mind the mind is a digital computer manipulating symbols.

But Searle argues that-

\* if there were a conversation between Searle, Computer and Chinese, where the Chinese describes a story of hamburger. All of them know the Chinese characters that represent the hamburger. But even if they all knew the characters, if the Chinese described the story of eating a hamburger in Chinese, neither the computer nor Searle would understand it.\
\* This is because the syntax is detached from the world. There is a (World-word) gap.\
\* Semantic knowledge is a (Language-world) relationship.\
\* For syntactic communication (common linguistic reference is needed)\
\* For Semantic communication (common and combined phenomenal and linguistic reference is needed)

Biological Naturalism

Neural states that underly thoughts carry representational content.

\*\*Bibliography\*\*

Kim, J. (2011) Philosophy of mind \~electronic resource\~ / Jaegwon Kim. 3rd ed. Boulder, CO: Boulder, CO: Westview Press.

Measure\
Measure

https://v3.pebblepad.co.uk/components/builder/blockHero/defaultImages/Pebbles.jpg

\#Philosophy of Mind\# \#Kim \#Turing Machine\# \#Searle \#Chinese Room Argument\# \#2011\# \#Computational Theory of Mind\# \#Turing Test\# \#CTM
